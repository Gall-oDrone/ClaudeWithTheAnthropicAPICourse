{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad14af6-faf1-4442-9f94-bda5e2d4370d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.chat_utils import text_to_json\n",
    "from prompts.eval_dataset_1 import EVALUATION_DATASET_1_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f36d11-8da7-473c-8cca-29db54e585b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.anthropic_client import ChatClient\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if api_key is None:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "\n",
    "messages = []\n",
    "stop_sequences = [\"```\"]\n",
    "\n",
    "params = {\n",
    "    \"messages\": messages,\n",
    "    \"stop_sequences\": stop_sequences,\n",
    "    \"max_tokens\": 4096,\n",
    "}\n",
    "\n",
    "# Create ChatClient\n",
    "chat_client = ChatClient(\n",
    "    api_key=api_key,\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# Create Evaluator for running evaluations\n",
    "evaluator = Evaluator(chat_client=chat_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad4478-0c75-48fe-b9b2-0303227719e1",
   "metadata": {},
   "source": [
    "## Generate Dataset (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110f4b95-57b5-426e-99b0-1dc1706d6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_DATASET = False\n",
    "if GENERATE_DATASET:\n",
    "    prompt = EVALUATION_DATASET_1_PROMPT\n",
    "    text = \"```json\"\n",
    "    dataset = chat_client.generate_dataset(\n",
    "        prompt=EVALUATION_DATASET_1_PROMPT, \n",
    "        text=text, \n",
    "        save_path=\"datasets/generated_dataset.json\"\n",
    "    )\n",
    "    print(f\"Generated dataset with {len(dataset)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b53255-ddd2-4227-97b0-85f8340156e2",
   "metadata": {},
   "source": [
    "## Running the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7bbc56-254f-404e-9fb7-d002e45d0c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test case 1/3...\n",
      "  ✗ Error in test case 1: 'AnthropicClient' object has no attribute 'params'\n",
      "Running test case 2/3...\n",
      "  ✗ Error in test case 2: 'AnthropicClient' object has no attribute 'params'\n",
      "Running test case 3/3...\n",
      "  ✗ Error in test case 3: 'AnthropicClient' object has no attribute 'params'\n",
      "\n",
      "Results saved to eval_results.json\n",
      "\n",
      "Evaluation complete: 0/3 tests passed (0.00%)\n",
      "Success! Passed 0/3 tests\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple dataset\n",
    "test_dataset = [\n",
    "    {\"prompt\": \"Write a hello world function\", \"expected_response\": \"A function that prints hello world\"},\n",
    "    {\"prompt\": \"What is 2+2?\", \"expected_response\": \"4\"},\n",
    "    {\"prompt\": \"Explain Python lists\", \"expected_response\": \"Explanation of Python lists\"}\n",
    "]\n",
    "\n",
    "# This should now work without the formatting error!\n",
    "results = evaluator.run_eval(test_dataset)\n",
    "print(f\"Success! Passed {results['passed_tests']}/{results['total_tests']} tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b57c489-f9ab-4852-8a5e-90eba6ca51b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test case 1/3...\n",
      "  ✗ Error in test case 1: 'AnthropicClient' object has no attribute 'params'\n",
      "Running test case 2/3...\n",
      "  ✗ Error in test case 2: 'AnthropicClient' object has no attribute 'params'\n",
      "Running test case 3/3...\n",
      "  ✗ Error in test case 3: 'AnthropicClient' object has no attribute 'params'\n",
      "\n",
      "Results saved to eval_results.json\n",
      "\n",
      "Evaluation complete: 0/3 tests passed (0.00%)\n",
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Total Tests: 3\n",
      "Passed: 0\n",
      "Failed: 3\n",
      "Pass Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"datasets/generated_dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Run evaluation using the Evaluator class\n",
    "results = evaluator.run_eval(\n",
    "    test_dataset=dataset,\n",
    "    save_results=True,\n",
    "    results_path=\"eval_results.json\"\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Tests: {results['total_tests']}\")\n",
    "print(f\"Passed: {results['passed_tests']}\")\n",
    "print(f\"Failed: {results['failed_tests']}\")\n",
    "print(f\"Pass Rate: {results['pass_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f360328-d883-4324-a2f4-13959c8f3403",
   "metadata": {},
   "source": [
    "## Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe8240c-ea14-4ce2-90c1-fc87effcefbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FAILED TEST CASES\n",
      "==================================================\n",
      "\n",
      "Prompt: Write a Python function that retrieves the list of EC2 instances in a specific AWS region....\n",
      "Error: 'AnthropicClient' object has no attribute 'params'\n",
      "\n",
      "Prompt: Create a JSON object that represents an AWS Lambda function configuration, including the function na...\n",
      "Error: 'AnthropicClient' object has no attribute 'params'\n",
      "\n",
      "Prompt: Write a regular expression to validate an AWS S3 bucket name....\n",
      "Error: 'AnthropicClient' object has no attribute 'params'\n"
     ]
    }
   ],
   "source": [
    "# Analyze specific failures\n",
    "if results['failed_tests'] > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FAILED TEST CASES\")\n",
    "    print(\"=\"*50)\n",
    "    for result in results['results']:\n",
    "        if not result.get('passed', False):\n",
    "            test_case = result.get('test_case', {})\n",
    "            print(f\"\\nPrompt: {test_case.get('prompt', 'N/A')[:100]}...\")\n",
    "            if 'error' in result:\n",
    "                print(f\"Error: {result['error']}\")\n",
    "            else:\n",
    "                grading = result.get('grading_results', {})\n",
    "                if 'code_grader' in grading:\n",
    "                    print(f\"Code Grader: {grading['code_grader'].feedback}\")\n",
    "                if 'model_grader' in grading:\n",
    "                    print(f\"Model Grader: {grading['model_grader'].feedback}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
