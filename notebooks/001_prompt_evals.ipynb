{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad14af6-faf1-4442-9f94-bda5e2d4370d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path (since notebooks don't have __file__)\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith('notebooks'):\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from utils.chat_utils import text_to_json\n",
    "from prompts.eval_dataset import EVALUATION_DATASET_1_PROMPT, EVALUATION_DATASET_2_PROMPT, EVALUATION_DATASET_3_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f36d11-8da7-473c-8cca-29db54e585b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.anthropic_client import ChatClient\n",
    "from utils.evaluator import Evaluator, FormatAwareEvaluator\n",
    "\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if api_key is None:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "\n",
    "messages = []\n",
    "stop_sequences = [\"```\"]\n",
    "max_tokens = 4096\n",
    "\n",
    "params = {\n",
    "    \"messages\": messages,\n",
    "    \"stop_sequences\": stop_sequences,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# Create ChatClient\n",
    "chat_client = ChatClient(\n",
    "    api_key=api_key,\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# Create Evaluator for running evaluations\n",
    "evaluator = Evaluator(chat_client=chat_client)\n",
    "\n",
    "# Create Format Aware Evaluator\n",
    "format_evaluator = FormatAwareEvaluator(chat_client=chat_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad4478-0c75-48fe-b9b2-0303227719e1",
   "metadata": {},
   "source": [
    "## Generate Dataset (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110f4b95-57b5-426e-99b0-1dc1706d6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_DATASET = False\n",
    "GENERATE_DATASET_FORMAT_SPECIFICATIONS = False\n",
    "if GENERATE_DATASET:\n",
    "    prompt = EVALUATION_DATASET_2_PROMPT\n",
    "    text = \"```code\"\n",
    "    dataset = chat_client.generate_dataset(\n",
    "        prompt=prompt, \n",
    "        text=text, \n",
    "        save_path=\"datasets/generated_dataset_format.json\"\n",
    "    )\n",
    "    print(f\"Generated dataset with {len(dataset)} test cases\")\n",
    "if GENERATE_DATASET_FORMAT_SPECIFICATIONS:\n",
    "    prompt = EVALUATION_DATASET_3_PROMPT\n",
    "    text = \"```code\"\n",
    "    dataset = chat_client.generate_dataset(\n",
    "        prompt=prompt, \n",
    "        text=text, \n",
    "        save_path=\"datasets/generated_dataset_with_format_specifications.json\"\n",
    "    )\n",
    "    print(f\"Generated dataset with {len(dataset)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b53255-ddd2-4227-97b0-85f8340156e2",
   "metadata": {},
   "source": [
    "## Running the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7bbc56-254f-404e-9fb7-d002e45d0c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test case 1/3...\n",
      "  âœ— Test case 1 failed\n",
      "Running test case 2/3...\n",
      "  âœ“ Test case 2 passed\n",
      "Running test case 3/3...\n",
      "  âœ“ Test case 3 passed\n",
      "\n",
      "Results saved to eval_results.json\n",
      "\n",
      "Evaluation complete: 2/3 tests passed (66.67%)\n",
      "Success! Passed 2/3 tests\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple dataset\n",
    "test_dataset = [\n",
    "    {\"prompt\": \"Write a hello world function\", \"solution_criteria\": \"A function that prints hello world\"},\n",
    "    {\"prompt\": \"What is 2+2?\", \"solution_criteria\": \"4\"},\n",
    "    {\"prompt\": \"Explain Python lists\", \"solution_criteria\": \"Explanation of Python lists\"}\n",
    "]\n",
    "\n",
    "# This should now work without the formatting error!\n",
    "results = evaluator.run_eval(test_dataset)\n",
    "print(f\"Success! Passed {results['passed_tests']}/{results['total_tests']} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34a975-b859-4aaa-babd-6cddfb806ccd",
   "metadata": {},
   "source": [
    "## Running the Evaluation with Format Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b250a36-6416-484a-82ee-05a7f7f07f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running Format-Aware Evaluation\n",
      "==================================================\n",
      "\n",
      "Test 1/3: Write a Python function that retrieves the list of EC2 insta...\n",
      "Format: python\n",
      "âŒ FAILED\n",
      "  Code Score: 10/10\n",
      "    Issue: Missing required words: def, return; Unsupported language: text\n",
      "  Model Score: 9/10\n",
      "\n",
      "Test 2/3: Create a JSON object that represents an AWS Lambda function ...\n",
      "Format: json\n",
      "âŒ FAILED\n",
      "  Code Score: 10/10\n",
      "    Issue: Output too short. Minimum length: 100, got: 89; Missing required words: def, boto3, ec2, return\n",
      "  Format Score: 8.0/10\n",
      "    Issue: Invalid JSON format: Expecting value: line 1 column 1 (char 0)\n",
      "  Model Score: 8.8/10\n",
      "\n",
      "Test 3/3: Write a regular expression to validate an AWS S3 bucket name...\n",
      "Format: regex\n",
      "âœ… PASSED\n",
      "\n",
      "Results saved to ../evaluation_results/enhanced_eval_results.json\n",
      "\n",
      "==================================================\n",
      "Format-Aware Evaluation Complete: 1/3 tests passed (33.33%)\n",
      "==================================================\n",
      "ðŸš€ Running Format-Aware Evaluation\n",
      "==================================================\n",
      "\n",
      "Test 1/3: Write a Python function that retrieves the list of EC2 insta...\n",
      "Format: python\n",
      "âŒ FAILED\n",
      "  Code Score: 10/10\n",
      "    Issue: Missing required words: def, return; Unsupported language: text\n",
      "  Model Score: 9/10\n",
      "\n",
      "Test 2/3: Create a JSON object that represents an AWS Lambda function ...\n",
      "Format: json\n",
      "âŒ FAILED\n",
      "  Code Score: 10/10\n",
      "    Issue: Output too short. Minimum length: 100, got: 89; Missing required words: def, boto3, ec2, return\n",
      "  Format Score: 8.0/10\n",
      "    Issue: Invalid JSON format: Expecting value: line 1 column 1 (char 0)\n",
      "  Model Score: 8.8/10\n",
      "\n",
      "Test 3/3: Write a regular expression to validate an AWS S3 bucket name...\n",
      "Format: regex\n",
      "âœ… PASSED\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š EVALUATION SUMMARY\n",
      "==================================================\n",
      "Overall Pass Rate: 1/3 (33.3%)\n",
      "\n",
      "ðŸ“ˆ Results by Format:\n",
      "  python: 0/1 passed (0.0%)\n",
      "  json: 0/1 passed (0.0%)\n",
      "  regex: 1/1 passed (100.0%)\n",
      "\n",
      "âš–ï¸ Results by Grader:\n",
      "  code_grader:\n",
      "    Average Score: 9.67/10\n",
      "    Pass Rate: 33.3%\n",
      "    Used in: 3 tests\n",
      "  format_grader:\n",
      "    Average Score: 7.00/10\n",
      "    Pass Rate: 0.0%\n",
      "    Used in: 2 tests\n",
      "  model_grader:\n",
      "    Average Score: 9.27/10\n",
      "    Pass Rate: 100.0%\n",
      "    Used in: 3 tests\n",
      "\n",
      "============================================================\n",
      "âœ… Enhanced evaluation completed!\n",
      "Check 'enhanced_eval_results.json' for detailed results.\n",
      "\n",
      "ðŸ“Š Additional Statistics Analysis:\n",
      "Total tests: 3\n",
      "Passed: 1\n",
      "Failed: 2\n",
      "\n",
      "Format breakdown:\n",
      "  python: 0.0% pass rate\n",
      "  json: 0.0% pass rate\n",
      "  regex: 100.0% pass rate\n"
     ]
    }
   ],
   "source": [
    "format_aware_dataset = [\n",
    "    {\n",
    "        \"prompt\": \"Write a Python function that retrieves the list of EC2 instances in a specific AWS region\",\n",
    "        \"format\": \"python\",\n",
    "        \"solution_criteria\": \"A Python function using boto3 to list EC2 instances\",\n",
    "        \"grading_config\": {\n",
    "            \"code\": {\n",
    "                \"min_length\": 100,\n",
    "                \"required_words\": [\"def\", \"boto3\", \"ec2\", \"return\"],\n",
    "                \"syntax_check\": True\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Create a JSON object that represents an AWS Lambda function configuration\",\n",
    "        \"format\": \"json\", \n",
    "        \"solution_criteria\": \"A valid JSON object with Lambda configuration properties\",\n",
    "        \"grading_config\": {\n",
    "            \"format\": {\n",
    "                \"required_fields\": [\"FunctionName\", \"Runtime\", \"Handler\", \"Role\"],\n",
    "                \"forbidden_fields\": [\"AccessKeyId\", \"SecretAccessKey\"],\n",
    "                \"validate_json_schema\": True,\n",
    "                \"json_schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"FunctionName\": {\"type\": \"string\"},\n",
    "                        \"Runtime\": {\"type\": \"string\"},\n",
    "                        \"Handler\": {\"type\": \"string\"},\n",
    "                        \"Role\": {\"type\": \"string\"},\n",
    "                        \"MemorySize\": {\"type\": \"number\"},\n",
    "                        \"Timeout\": {\"type\": \"number\"}\n",
    "                    },\n",
    "                    \"required\": [\"FunctionName\", \"Runtime\", \"Handler\", \"Role\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a regular expression to validate an AWS S3 bucket name\",\n",
    "        \"format\": \"regex\",\n",
    "        \"solution_criteria\": \"A regex pattern that validates S3 bucket naming rules\",\n",
    "        \"grading_config\": {\n",
    "            \"code\": {\n",
    "                \"min_length\": 20,\n",
    "                \"syntax_check\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = format_evaluator.run_format_aware_eval_with_detailed_display(\n",
    "        test_dataset=format_aware_dataset,\n",
    "        save_results=True,\n",
    "        results_path=\"../evaluation_results/enhanced_eval_results.json\",\n",
    "        verbose=True,\n",
    "        show_individual_tests=True,\n",
    "        max_display=5\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Enhanced evaluation completed!\")\n",
    "print(\"Check 'enhanced_eval_results.json' for detailed results.\")\n",
    "    \n",
    "# Demonstrate statistics calculation separately\n",
    "print(\"\\nðŸ“Š Additional Statistics Analysis:\")\n",
    "stats = format_evaluator.calculate_format_statistics(results['results'])\n",
    "    \n",
    "print(f\"Total tests: {stats['overall']['total']}\")\n",
    "print(f\"Passed: {stats['overall']['passed']}\")\n",
    "print(f\"Failed: {stats['overall']['failed']}\")\n",
    "    \n",
    "print(\"\\nFormat breakdown:\")\n",
    "for format_type, format_stats in stats['by_format'].items():\n",
    "    total = format_stats['passed'] + format_stats['failed']\n",
    "    pass_rate = format_stats['passed'] / total * 100 if total > 0 else 0\n",
    "    print(f\"  {format_type}: {pass_rate:.1f}% pass rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57c489-f9ab-4852-8a5e-90eba6ca51b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test case 1/3...\n",
      "  âœ— Test case 1 failed\n",
      "Running test case 2/3...\n",
      "  âœ— Test case 2 failed\n",
      "Running test case 3/3...\n",
      "  âœ— Test case 3 failed\n",
      "\n",
      "Results saved to eval_results.json\n",
      "\n",
      "Evaluation complete: 0/3 tests passed (0.00%)\n",
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Total Tests: 3\n",
      "Passed: 0\n",
      "Failed: 3\n",
      "Pass Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"datasets/generated_dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Run evaluation using the Evaluator class\n",
    "results = evaluator.run_eval(\n",
    "    test_dataset=dataset,\n",
    "    save_results=True,\n",
    "    results_path=\"../evaluation_results/eval_results.json\"\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Tests: {results['total_tests']}\")\n",
    "print(f\"Passed: {results['passed_tests']}\")\n",
    "print(f\"Failed: {results['failed_tests']}\")\n",
    "print(f\"Pass Rate: {results['pass_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f360328-d883-4324-a2f4-13959c8f3403",
   "metadata": {},
   "source": [
    "## Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe8240c-ea14-4ce2-90c1-fc87effcefbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FAILED TEST CASES\n",
      "==================================================\n",
      "\n",
      "Prompt: Write a Python function that retrieves the list of EC2 instances in a specific AWS region....\n",
      "Code Grader: Unsupported language: text\n",
      "Model Grader: The response provides an excellent Python function that retrieves the list of EC2 instances in a specific AWS region. It is well-written, comprehensive, and follows the given instructions closely. This function would be very useful for AWS administrators and developers who need to interact with EC2 instances programmatically.\n",
      "\n",
      "Prompt: Create a JSON object that represents an AWS Lambda function configuration, including the function na...\n",
      "Code Grader: Unsupported language: text\n",
      "Model Grader: The response is of high quality, as it addresses the question/task well, follows the instructions, is complete, helpful, and safe. The provided JSON object is a clear and accurate representation of an AWS Lambda function configuration.\n",
      "\n",
      "Prompt: Write a regular expression to validate an AWS S3 bucket name....\n",
      "Code Grader: Unsupported language: text\n",
      "Model Grader: The response is completely inadequate as it does not provide the requested regular expression to validate an AWS S3 bucket name. The response fails to address the question and does not follow the given instructions.\n"
     ]
    }
   ],
   "source": [
    "# Analyze specific failures\n",
    "if results['failed_tests'] > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FAILED TEST CASES\")\n",
    "    print(\"=\"*50)\n",
    "    for result in results['results']:\n",
    "        if not result.get('passed', False):\n",
    "            test_case = result.get('test_case', {})\n",
    "            print(f\"\\nPrompt: {test_case.get('prompt', 'N/A')[:100]}...\")\n",
    "            if 'error' in result:\n",
    "                print(f\"Error: {result['error']}\")\n",
    "            else:\n",
    "                grading = result.get('grading_results', {})\n",
    "                if 'code_grader' in grading:\n",
    "                    print(f\"Code Grader: {grading['code_grader'].feedback}\")\n",
    "                if 'model_grader' in grading:\n",
    "                    print(f\"Model Grader: {grading['model_grader'].feedback}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claude-with-the-anthropic-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
